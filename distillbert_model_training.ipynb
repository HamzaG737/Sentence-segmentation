{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TddoDD6JU2rU"
   },
   "source": [
    "## Load librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciuLNyIIU4Iq",
    "outputId": "9e1c21de-e9d2-45e9-c91e-accb2bb1b5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P199oD_MVOAx",
    "outputId": "6e83438d-ae1e-4567-a36b-7bf02a7ce810"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "20e3mXgjU2re"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import average_precision_score,f1_score,recall_score\n",
    "import pickle\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m_PBEnAcU2rf"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2H_vTWmvU2rg",
    "outputId": "3638024f-1ce9-41ce-807a-5b4bcff68774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available \n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh5UBunkU2rj"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mgJL9UbFU2rk"
   },
   "outputs": [],
   "source": [
    "sentences_data = pickle.load(open('data/tatoeba_sentences.pkl','rb'))\n",
    "labels_data = pickle.load(open('data/tatoeba_grouped_labels.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLfpHS6tU2rk"
   },
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uWoj1mL5e3uD"
   },
   "outputs": [],
   "source": [
    "def encode_tags(labels, encodings):\n",
    "    \"\"\"\n",
    "    Function that adds -100 labels to subtokens or to <pad> and <cls> tokens . These artificial labels will be masked at training.\n",
    "    More details can be found here https://huggingface.co/transformers/custom_datasets.html , \"Token Classification with W-NUT Emerging Entities\"\n",
    "    \"\"\"\n",
    "    encoded_labels,index = [],[]\n",
    "    for i,(doc_labels, doc_offset) in enumerate(zip(labels, encodings.offset_mapping)):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        try :  \n",
    "          doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        except :\n",
    "          pass\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "    return encoded_labels\n",
    "\n",
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Define class for creating torch dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xi4O2XtOU2rl"
   },
   "outputs": [],
   "source": [
    "def tokenization(sentences,labels):\n",
    "    \"\"\"\n",
    "    This function tokenize every sentence of the training set, than adds masks to ensure padding. \n",
    "    \"\"\"\n",
    "    encodings = tokenizer(sentences, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "    label_encodings = encode_tags (labels,encodings)\n",
    "\n",
    "    return encodings,label_encodings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NC_UngcJU2rm"
   },
   "outputs": [],
   "source": [
    "def getting_loader(sentences,labels) :\n",
    "    \"\"\"\n",
    "    Creates train and validation loaders, the ratio of validation is 10% of all examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_sents, validation_sents, train_labels, validation_labels = train_test_split(sentences, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "\n",
    "    train_inputs , train_enc_labels = tokenization(train_sents,train_labels)\n",
    "    val_inputs , val_enc_labels = tokenization(validation_sents,validation_labels)\n",
    "    \n",
    "\n",
    "    train_inputs.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "    val_inputs.pop(\"offset_mapping\")\n",
    "    ## create datasets\n",
    "    train_dataset = WNUTDataset(train_inputs, train_enc_labels)\n",
    "    val_dataset = WNUTDataset(val_inputs, val_enc_labels)\n",
    "\n",
    "    batch_size = 8\n",
    "    ## create loader for training set\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32 , sampler = train_sampler)\n",
    "\n",
    "    # Create the DataLoader for our validation set.\n",
    "    validation_sampler = torch.utils.data.SequentialSampler(val_dataset)\n",
    "    validation_dataloader = torch.utils.data.DataLoader(val_dataset, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    return train_dataloader,validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xUHL6-iDU2rn"
   },
   "outputs": [],
   "source": [
    "def get_scores(preds, labels):\n",
    "    \"\"\"\n",
    "    returns f1-score, average precision and recall given the true labels and the predictions of the distillbert model.\n",
    "    \"\"\"\n",
    "    indexes = np.where(labels != -100)\n",
    "    preds = preds[indexes]\n",
    "    labels = labels[indexes]\n",
    "    f1score = f1_score(preds, labels )\n",
    "    avg_prec_score = average_precision_score(preds, labels)\n",
    "    recall = recall_score(preds,labels)\n",
    "    d_score = {'f1':f1score,'avg_precision':avg_prec_score,'recall':recall}\n",
    "    return d_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6yYhgLEcU2ro"
   },
   "outputs": [],
   "source": [
    "def training(train_loader,epochs,optimizer,model,scheduler) :\n",
    "    \"\"\"\n",
    "    Trains the model and evaluate at the end of each epoch.\n",
    "    \"\"\"\n",
    "    loss_values = []\n",
    "\n",
    "    # For each epoch...\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "    \n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            \n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_loader)            \n",
    "    \n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        \n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for l,batch in enumerate(validation_loader):\n",
    "        \n",
    "            # Add batch to GPU\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['labels'].to(device)\n",
    "        \n",
    "        \n",
    "            # Telling the model not to compute or store gradients, saving memory and\n",
    "            # speeding up validation\n",
    "            with torch.no_grad():        \n",
    "                outputs = model(b_input_ids, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "            logits = outputs[0]\n",
    "            \n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            \n",
    "            ## stack all predictions and update scores at the end of the epoch.\n",
    "            if l == 0 : \n",
    "              all_pred_flat = np.argmax(logits, axis=2).flatten()\n",
    "              all_labels_flat = label_ids.flatten()\n",
    "            else :\n",
    "              pred_flat = np.argmax(logits, axis=2).flatten()\n",
    "              labels_flat = label_ids.flatten()\n",
    "              all_pred_flat  = np.hstack((all_pred_flat,pred_flat))\n",
    "              all_labels_flat = np.hstack((all_labels_flat,labels_flat))\n",
    "            \n",
    "        eval_scores = get_scores(all_pred_flat, all_labels_flat)\n",
    "        torch.save(model.state_dict(), 'models/distillbert_model_glose_finetuned_{}.pth'.format(str(epoch_i+2)))\n",
    "        print('F1-score : {}, average precision : {}, recall : {}'.format(eval_scores['f1'],eval_scores['avg_precision'],eval_scores['recall']))\n",
    "\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Etv--uc8U2rp"
   },
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "f36b4027f5c64aedad07998908feccbc",
      "25c88bbf60d4429f863d58356d1e12c9",
      "69d113b4efc24cfbad17c8d6d6a49ecb",
      "b847f429b57f423da2f715499aa5a7ec",
      "476ab481915b46a186db077e7f1db36d",
      "a56fabbf68b24add9e10ab54f9a69061",
      "384ba8adc0904d3a93c33db9ed87cde8",
      "c7f45af224054024bcaa60cfd73b296c",
      "60039b4428d3428080e9549b1c9b8c25",
      "e5aff5bb5aaa4d0fbe16bea32e5deb09",
      "09dfd1165334448ba95c7d395a5eed9e",
      "f83fd6038c6a44fdb57d696e772ce661",
      "0070036efc954c62a46e845f07d40bf6",
      "afbf3b7bf08c4557a5f85f5416ba4984",
      "9ea9267db958478c913ccb6e793a7867",
      "c944b83ff39d49b69196316fa6b59a57"
     ]
    },
    "id": "IjgSyY6pU2rq",
    "outputId": "7703d420-3516-4e37-9761-4e0208a14354"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36b4027f5c64aedad07998908feccbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60039b4428d3428080e9549b1c9b8c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Defining epochs hyperparameters\n",
    "epochs=5\n",
    "\n",
    "## Defining the tokenizer and the pretrained weights of the distillbert model\n",
    "tokenizer_class, pretrained_weights = (ppb.DistilBertTokenizerFast,'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XSPLmKPNU2rr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5aa0edbf-ed11-477d-c677-8a128fda7b8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Defining the model : we must specify that it is the model for token classification. \n",
    "\n",
    "model = ppb.DistilBertForTokenClassification.from_pretrained(\n",
    "    pretrained_weights, # Use the distillbert model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)\n",
    "\n",
    "## load pretrained model\n",
    "PATH = '/content/drive/MyDrive/glose/distillbert_model_glose_finetuned_3.pth'\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aToiaRj1U1dV"
   },
   "source": [
    "I trained the model only with 40000 examples. With this number the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hbQRelFEU2rr",
    "outputId": "1ef3c73b-e3c2-4b5f-e269-75d43fdec1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k number  5\n",
      "k number  0\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.03\n",
      "\n",
      "Running Validation...\n",
      "F1-score : 0.9235586870211933, average precision : 0.8601030323886507, recall : 0.9102733686067019\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Running Validation...\n",
      "F1-score : 0.9261238957969954, average precision : 0.8640268459676812, recall : 0.9183218031689355\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Running Validation...\n",
      "F1-score : 0.9254212780156832, average precision : 0.8640145579218905, recall : 0.9073064340239912\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.02\n",
      "\n",
      "Running Validation...\n",
      "F1-score : 0.9245009479201517, average precision : 0.8620838933904672, recall : 0.908692316124082\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-70515bdc7eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                             num_training_steps = total_steps)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-78a2577e3e35>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(train_loader, epochs, optimizer, model, scheduler)\u001b[0m\n\u001b[1;32m     33\u001b[0m             outputs = model(b_input_ids, \n\u001b[1;32m     34\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                         labels=b_labels)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 active_labels = torch.where(\n\u001b[0;32m--> 812\u001b[0;31m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m                 )\n\u001b[1;32m    814\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader,validation_loader=getting_loader(sentences_data[:40000],labels_data[:40000])\n",
    "\n",
    "\n",
    "optimizer =AdamW(model.parameters(), ## Creating the optimizer \n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "            \n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_loader) * epochs\n",
    "            \n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "            \n",
    "model=training(train_loader,epochs,optimizer,model,scheduler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqY_0xyS5YqH"
   },
   "source": [
    "Best performance on validation is on epoch 2. The scores are : <br>\n",
    "F1-score : 0.9235586870211933, average precision : 0.8601030323886507, recall : 0.9102733686067019. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2liJhmI3U2rr"
   },
   "outputs": [],
   "source": [
    "## Saves the model as pth file \n",
    "#torch.save(model.state_dict(), '/content/drive/MyDrive/glose/distillbert_model_glose_finetuned_final.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYSTgJv7NpaN"
   },
   "source": [
    "# Evaluation on one text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FQI1Tnguf2jQ"
   },
   "outputs": [],
   "source": [
    "def match_sent(sentences , output_path):\n",
    "  \"\"\"\n",
    "  The reconstruction of sentences with distilbert tokenization to recover \n",
    "  the original sentence can't be done with just \"join\". So we add some preprocessing\n",
    "  steps to recover approximately the same sentences. Note that we will lose some\n",
    "  properties such as capital letters. \n",
    "  This function adds also spans + save all sentences in a text file.\n",
    "  \"\"\"\n",
    "  sentences = [sentence.replace(' ##','') for sentence in sentences]\n",
    "  sentences = [sentence.replace(\" ' \",\"'\") for sentence in sentences]\n",
    "  sentences = [sentence.replace(\"did n't\",\"didn't\") for sentence in sentences]\n",
    "\n",
    "  ## add span \n",
    "  sentences = [\"<span>\"+sentence+'</span>' for sentence in sentences]\n",
    "  with open(output_path, 'w') as output:\n",
    "    for sentence in sentences:\n",
    "        output.write(sentence + '\\n')\n",
    "  return sentences\n",
    "def normalize (preds): \n",
    "  \"\"\"\n",
    "  fonction that replaces 11 (i.e two adjacent tokens that both represent the ending of a sentence) \n",
    "  with 10 to avoid errors.\n",
    "  \"\"\"\n",
    "  l = list(preds)\n",
    "  string_list = ''.join(map(str,l))\n",
    "  string_list = string_list.replace('11', '01')\n",
    "  new_preds = np.array(list(map(int, list(string_list))))\n",
    "  return new_preds\n",
    "def get_sentences (indexes_end,tokens_recov,sentences) :\n",
    "  \"\"\"\n",
    "  given the indexes of tokens that end sentences and the list of all the tokens ,\n",
    "  This function gives the list of all sentences contained in window_sentences.\n",
    "  \"\"\"\n",
    "  current = []\n",
    "  for k in range(len(tokens_recov)) :\n",
    "    current.append(tokens_recov[k])\n",
    "    if k in indexes_end :\n",
    "      sentences.append(\" \".join(current))\n",
    "      current = []\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZGOTps5Nr3i",
    "outputId": "aae39a74-0160-448a-fec2-22df82eedaff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "57\n",
      "['<span>the first pig was very lazy</span>', \"<span>he didn't want to work</span>\", '<span>and he built his house out of straw</span>', '<span>the second pig worked a little bit harder but he was somewhat lazy too</span>', '<span>he built his house out of sticks .</span>', '<span>then , they sang , danced and played together the rest of the day .</span>']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from scipy.special import softmax\n",
    "\n",
    "nltk.download('punkt')\n",
    "full_text = \"The first  pig was very lazy he didn't want to work  and he built his house out of straw the second pig worked a little bit harder but he was somewhat lazy too he built his house out of sticks. Then, they sang, danced and played together the rest of the day.\"\n",
    "\n",
    "\n",
    "tokenized_text = nltk.word_tokenize(full_text.lower()) ## tokenize all text with nltk\n",
    "model.eval()\n",
    "\n",
    "sentences = []\n",
    "max_length = 10 ## size of sliding window\n",
    "current_begin = 0 ## beginning index of window_sentences , relative to tokenized_text.\n",
    "moving_add = 0 ## we will use this if window_sentences is an unfinished sentence.\n",
    "window_sentences = tokenized_text[:max_length]\n",
    "j,t=0,0\n",
    "while len(window_sentences) !=0 : \n",
    "  j+=1\n",
    "  inputs_enc = tokenizer(window_sentences, is_split_into_words= True, return_offsets_mapping=False, \n",
    "                       padding=False, truncation=True)\n",
    "  with torch.no_grad():     \n",
    "    input_ids_ = torch.tensor(inputs_enc.input_ids).unsqueeze(0).to(device)\n",
    "    outputs = model(input_ids_)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "  preds = np.argmax(logits, axis=2).flatten()[1:-1] ## take all except cls and sep preds\n",
    "  \n",
    "  preds = normalize(preds)\n",
    "  tokens_recov = tokenizer.convert_ids_to_tokens(inputs_enc['input_ids'])[1:-1]\n",
    "  \n",
    "  ## get the indexes of elements that end sentences\n",
    "  indexes_end = np.where(preds==1)[0]\n",
    "  sentences = get_sentences (indexes_end,tokens_recov ,sentences)\n",
    "  \n",
    "\n",
    "  if len(indexes_end)==1 : # if we have only one ending token , in the end of the sentence\n",
    "      ## this case means that there is no ending token except the default last one, \n",
    "      ## so we add 10 tokens to sentences test\n",
    "      moving_add +=10  \n",
    "\n",
    "      ## we stop if we exceed tokenized_text twice.\n",
    "      if current_begin+max_length+moving_add>len(tokenized_text): \n",
    "        t+=1\n",
    "        if t == 2 : \n",
    "          break \n",
    "      window_sentences = tokenized_text[current_begin:current_begin+max_length+moving_add]\n",
    "      sentences.pop(-1)\n",
    "      continue\n",
    "      \n",
    "      #current_begin += max_length\n",
    "\n",
    "  moving_add=0\n",
    "  \n",
    "  ## this is in case we hove more than two ending tokens. \n",
    "  last_sent = sentences[-1] # we will remove last sentence.\n",
    "  first_token = sentences[-1].split()[0]\n",
    "  indexes_first = np.where(np.array(window_sentences) == first_token)[0]\n",
    "  for index in reversed(list(indexes_first)) : \n",
    "    if index<=(len(window_sentences)-len(sentences[-1].split())+4) :\n",
    "      index_first = index\n",
    "      break\n",
    "  \n",
    "  ## window_sentences will be defined as the window beginning from the last sentence and we add max_length tokens\n",
    "  window_sentences = tokenized_text[current_begin+index_first:current_begin+index_first+max_length]\n",
    "  if current_begin+index_first > len(tokenized_text) : \n",
    "    break\n",
    "  sentences.pop(-1)\n",
    "  current_begin += index_first\n",
    "sentences = match_sent(sentences,\"sentences.txt\")\n",
    "print(sentences)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "distillbert_model_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0070036efc954c62a46e845f07d40bf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "09dfd1165334448ba95c7d395a5eed9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afbf3b7bf08c4557a5f85f5416ba4984",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0070036efc954c62a46e845f07d40bf6",
      "value": 466062
     }
    },
    "25c88bbf60d4429f863d58356d1e12c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "384ba8adc0904d3a93c33db9ed87cde8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "476ab481915b46a186db077e7f1db36d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "60039b4428d3428080e9549b1c9b8c25": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09dfd1165334448ba95c7d395a5eed9e",
       "IPY_MODEL_f83fd6038c6a44fdb57d696e772ce661"
      ],
      "layout": "IPY_MODEL_e5aff5bb5aaa4d0fbe16bea32e5deb09"
     }
    },
    "69d113b4efc24cfbad17c8d6d6a49ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a56fabbf68b24add9e10ab54f9a69061",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_476ab481915b46a186db077e7f1db36d",
      "value": 231508
     }
    },
    "9ea9267db958478c913ccb6e793a7867": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a56fabbf68b24add9e10ab54f9a69061": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afbf3b7bf08c4557a5f85f5416ba4984": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b847f429b57f423da2f715499aa5a7ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7f45af224054024bcaa60cfd73b296c",
      "placeholder": "​",
      "style": "IPY_MODEL_384ba8adc0904d3a93c33db9ed87cde8",
      "value": " 232k/232k [07:40&lt;00:00, 502B/s]"
     }
    },
    "c7f45af224054024bcaa60cfd73b296c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c944b83ff39d49b69196316fa6b59a57": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5aff5bb5aaa4d0fbe16bea32e5deb09": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f36b4027f5c64aedad07998908feccbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_69d113b4efc24cfbad17c8d6d6a49ecb",
       "IPY_MODEL_b847f429b57f423da2f715499aa5a7ec"
      ],
      "layout": "IPY_MODEL_25c88bbf60d4429f863d58356d1e12c9"
     }
    },
    "f83fd6038c6a44fdb57d696e772ce661": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c944b83ff39d49b69196316fa6b59a57",
      "placeholder": "​",
      "style": "IPY_MODEL_9ea9267db958478c913ccb6e793a7867",
      "value": " 466k/466k [07:40&lt;00:00, 1.01kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
